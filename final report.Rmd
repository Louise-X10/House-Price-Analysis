---
title: "Final report "
author: "Asa Ferguson, Louise Xu, Xinran Liu"
output: pdf_document
date: "2022-12-04"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
library(tidyverse)
library(GGally)
library(skimr)
library(leaps) # regsubsets: subset selection
library(caret)
library(yardstick)
library(rpart) # trees
library(randomForest)
library(gglasso)
library(glmnet)
```

# Abstract

We will be investigating the prediction of house prices using recent data from Ames Iowa. We are interested in finding which features are the most relevant in predicting house sale price. We implement several methods of analysis, such as linear regression with subset selection, linear regression with LASSO regularization, and various tree-based methods including random forests. We found that well-tuned random forest and LASSO regression on log-transformed data produce the best results on test data, with the LASSO method performing surprisingly well in comparison with random forest.

# Introduction

The housing market has always been a trending topic; people are sensitive about how tight the current sales market is and whether it is a good time to buy or sell. According to the PD&R, there are 6120 thousand existing home sales and 771 thousand new home sales in 2021. The average sale price is $347$ thousand and 397 thousand for each, approximately 20% higher than the price in 2020. With this highly active and fluctuating market, researchers are making lots of forecasts of pricing, supply, and demand every day based on economic conditions and demographic trends. As for individual builders and mortgagees, housing inventory characteristics are crucial in determining the sale price for each specific property. Road access, configuration, and accessories all affect how much people are willing to pay. Uncovering the key features in determining house sale prices is a difficult task. But we are excited to give house price prediction a shot using the skills we have learned in Math 243.

We are excited to be working with a dataset that is more current and expansive than some of the other famous and accessible datasets containing home price information. As an example, the Boston House Prices dataset that is used in many statistics classes (we even used it in our class!) is sourced from data compiled in 1970. In addition to being a relatively recent dataset, the response variable, home sale price, is incredibly relevant to the lives of people all across the country. While none of us are in a position to buy a house, there are millions of people in the US who are either homeowners or are considering buying. Predicting house prices is useful not only for buyers looking for a new home, but also for owners evaluating their assets and deciding whether to sell. Any insight we gain into the factors that lead to an increase in home price will be exciting, in light of the importance of home pricing to so many people.

A precise statement of our research question is: Which features are the most relevant in predicting house sale price, and which type of model best predicts the house price as a function of those features?

# Methods

Our dataset is the Ames Housing dataset compiled by Dean De Cock for use in data science education. These data were gathered from Ames Iowa during a period spanning 2006 to 2010. The observational unit is a house sold in Ames from 2006 to 2010. The response variable, house sale price, is quantitative. The dataset includes 79 predictor features, with more than half of our predictors categorical. The predictors range from geographical information about the property and the neighborhood, to interior measurements and quality of amenities, to sales statistics. The dataset has 1460 observations. We found this dataset through the "House Prices - Advanced Regression Techniques" Kaggle competition, available [here](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/).

Several important steps were taken in order to prepare this dataframe for our statistical analysis. The first step was recoding NA's. In the raw .csv file, many of the categorical variables took the value NA frequently. Upon reading the codebook, it became clear that in many of the variables, the value NA actually stored meaning. Hence, whenever NA was listed as a possible value storing meaning for a categorical variable in the codebook, we replaced the value NA with the character value "None" so that our algorithms would not treat these NAs as missing data. Next, the continuous variable `LotFrontage` contained a large number of NA values. This variable records the length of street/sidewalk bordering a home. After some exploratory analysis, it became clear that instead of recording "no access" as 0, it was recorded as an NA. Hence, we recoded NA as 0 in LotFrontage. In any case where the codebook indicated that a variable which took numerical values should be interpreted as categorical (variables such as house legal subclass and month sold), we changed the variable type to character.

The next issue to be addressed was rare levels. Some of the categorical features contained levels that were exceedingly rare. In extreme cases, levels were observed only once or twice. This imbalance in levels created issues in data splitting both for cross validation and for the validation set. Since the problem was so acute, even stratified sampling was not an option. Our solution was to combine the rare levels into a single level "other." We set a threshold of 10 observations in a level. Levels with fewer than ten observations were collapsed into the single "other" category. This solution was designed to balance functionality with the desire to preserve the original data structure. Ten observations in each level gives an expected number of 2.5 observations in each level in the test set and 7.5 in the training set, which means that we likely will have at least one observation from each level in the training set and test set. When all levels within the test set are contained in the training set, our models will be able to make predictions on the test data, which will enable us to draw conclusions about relative model performance.

Even when combining rare levels into one "other" category, there were still categorical variables in which the other variable had less than 10 observations. These rare instances created the same issues with CV and in the training test split, so it was decided that these rare "other" values would be overwritten with the mode of the categorical variable. These rare observations reflect less than 0.68% of the observations in each level, so the alteration to the data from imputing was not at all substantial. To prevent data leakage, when we imputed the mode, we imputed the training mode for training observations and the test mode for test observations. If imputation created a variable that only took one single value, then that variable was removed from the whole dataset. This occurred for the variables `Street`, `Utilities`, and `PoolQC`.

After handling the rare values, there were still some variables which had to be cleaned up. The variable `GarageYrBlt` records the year that the home's garage was built. If the home did not have a garage, this value was stored as NA. These 81 NA's in our dataframe were causing issues in our regressions so we performed some exploratory data analysis to see if we could drop `GarageYrBlt`. We found that `GarageYrBlt` had over 0.8 correlation with `YearBuilt`. Further, the variable `GarageType` took the level "none" whenever a home was missing a garage. This meant that a large majority of the information present in `GarageYrBlt` was stored in other variables. Hence, we dropped `GarageYrBlt`. Garage-related variables still held more trouble. The variables `GarageArea` and `GarageCars` both stored the information of garage square footage. The only difference was the units of measure: one in square feet and the other in car capacity. Since car capacity is measured on a discrete scale, and square footage on a continuous scale, the correlation between the variables was not exactly one. Yet the correlation was still found to be almost 0.9 (indicating extreme redundancy), so the variable `GarageCars` was dropped. Next, the variables recording Masonry Veneer Type and Masonry Veneer Area had 8 missing values for the same 8 rows. The mode of "none" (across both the test and training sets!) was imputed for Veneer Type and the value corresponding to "none," 0, was imputed for the Veneer Area. Finally the one remaining categorical variable which truly did take NA values (the NA only stored the information that the data were missing), `Electrical`, took only one NA value and the mode value of the set it landed in was imputed here.

```{r}
train_raw <- read_csv("train.csv", show_col_types = FALSE)
```

```{r preprocess data, echo=FALSE, show_col_types = FALSE}
#Recode NAâ€™s as character values
train_df <- train_raw %>% 
  select(-Id) %>% 
  mutate(Alley = ifelse(is.na(Alley), "None", Alley),
         FireplaceQu = ifelse(is.na(FireplaceQu), "None", FireplaceQu),
         PoolQC = ifelse(is.na(PoolQC),  "None", PoolQC),
         Fence = ifelse(is.na(Fence), "None", Fence),
         MiscFeature = ifelse(is.na(MiscFeature), "None", MiscFeature),
         BsmtQual = ifelse(is.na(BsmtQual), "None", BsmtQual),
         BsmtCond = ifelse(is.na(BsmtCond), "None", BsmtCond),
         BsmtExposure = ifelse(is.na(BsmtExposure), "None", BsmtExposure),
         BsmtFinType1 = ifelse(is.na(BsmtFinType1), "None", BsmtFinType1),
         BsmtFinType2 = ifelse(is.na(BsmtFinType2), "None", BsmtFinType2),
         GarageType = ifelse(is.na(GarageType), "None", GarageType),
         GarageFinish = ifelse(is.na(GarageFinish), "None", GarageFinish),
         GarageQual = ifelse(is.na(GarageQual), "None", GarageQual),
         GarageCond = ifelse(is.na(GarageCond), "None", GarageCond),
         # YrSold = as.character(YrSold),
          MoSold = as.character(MoSold),
         # YearBuilt = as.character(YearBuilt),
         # YearRemodAdd = as.character(YearRemodAdd)
         MSSubClass = as.character(MSSubClass)
) %>%  
  rename("X1stFlrSF"= `1stFlrSF`) %>% 
  rename("X2ndFlrSF"= `2ndFlrSF`) %>% 
  rename("X3SsnPorch"= `3SsnPorch`)

# 0.8 correlation beweetn Garage Year built and House year build
# train_df %>% select(GarageYrBlt, YearBuilt) %>% drop_na() %>%  cor()

#Change NAs in LotFrontage to zeroes
train_df<-train_df %>% 
  mutate(LotFrontage = ifelse(is.na(LotFrontage), 0,LotFrontage)
) %>% 
  select(-GarageYrBlt) %>% 
  select(-GarageCars)
```

```{r otherify and modeify, echo=FALSE}
Mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}

otherify<-function(df, k, mode_rep){
  names<-df %>% 
    select_if(negate(is.numeric)) %>% 
    names()
  # d: each dummy counts number of obs in each level
  # names-vecs: each entry is a list of the levels with count <=k
  d<-list()
  names_vecs<-list()
  for (i in 1:length(names)){
    dummy<-df %>% 
      group_by(.[[paste(names[i])]]) %>% 
      summarise(count=n()) 
  names(dummy)=c(names[i],"count")
  
  d[[i]]<-dummy
  
  names_vecs[[i]]<-dummy %>% 
    filter(count<=k) %>% 
    select(-count) %>% 
    pull()
  }
  
  for (i in 1:length(names)){
    df[[names[i]]]=ifelse(df[[names[i]]]%in%names_vecs[[i]],"other",df[[names[i]]])
  }
  
  if (mode_rep==T){
    for (i in 1:length(names)){
      s<-sum(df[,names[i]]=="other")
      if ((s!=0)&(s<k)){
        df[[names[i]]]=ifelse(df[[names[i]]]=="other",
                         Mode(df[[names[i]]]),
                         df[[names[i]]])
      }
    }
  }
  
  return(df)
}

modeify<-function(df, k){
  names<-df %>% 
    select_if(negate(is.numeric)) %>% 
    names()
  for (i in 1:length(names)){
      s<-sum(df[,names[i]]=="other")
      if ((s!=0)&(s<=k)){
        df[[names[i]]]=ifelse(df[[names[i]]]=="other",
                         Mode(df[[names[i]]]),
                         df[[names[i]]])
      }
  }
  return(df)
}
```

```{r train test split, echo=FALSE}

train_df<-train_df %>% 
  otherify(k=10, mode_rep = F)

sample_size = floor(0.75*nrow(train_df))
set.seed(1)
picked = sample(seq_len(nrow(train_df)),size = sample_size)
train =train_df[picked,]
test =train_df[-picked,]

#Impute the mode value in train and test for those "other" categories with 
# less than 10 overall predictors
names<-train_df %>% 
    select_if(negate(is.numeric)) %>% 
    names()


for (i in 1:length(names)){
    s<-sum(train_df[[names[i]]]=="other")
    if ((s!=0)&(s<=10)){
      train[[names[i]]]=ifelse(train[[names[i]]]=="other",
                       Mode(train[[names[i]]]),
                       train[[names[i]]])
      test[[names[i]]]=ifelse(test[[names[i]]]=="other",
                       Mode(test[[names[i]]]),
                       test[[names[i]]])
    }
  }

#remove singleton categoricals
singleton<-character()
t=1
# s = number of others in train_df
# l = number of unique values in train_df
for (i in 1:length(names)){
  s<-sum(train_df[[names[i]]]=="other")
  l<-length(train_df[[names[i]]] %>% unique())
  if (((s!=0)&(s<=10)&(l==2))|| (l==1)  ){
    singleton[t]=names[i]
    t=t+1
  }
}

train_df<-train_df %>% 
  select(-all_of(singleton))
train<-train %>% 
  select(-all_of(singleton))
test<-test %>% 
  select(-all_of(singleton))


#Impute missing values for MasVnr and Electrical and perform rough fix for GarageYrBlt
train<-train %>% 
  mutate(MasVnrType=ifelse(is.na(MasVnrType), "None",MasVnrType)) %>% 
  mutate(MasVnrArea=ifelse(is.na(MasVnrArea), 0,MasVnrArea)) %>% 
  mutate(Electrical=ifelse(is.na(Electrical),"SBrkr",Electrical)) 

#Again for the test set, where we checked that the mode is the same
test<-test %>% 
  mutate(MasVnrType=ifelse(is.na(MasVnrType), "None",MasVnrType)) %>% 
  mutate(MasVnrArea=ifelse(is.na(MasVnrArea), 0,MasVnrArea)) %>% 
  mutate(Electrical=ifelse(is.na(Electrical),"SBrkr",Electrical))

```

# Exploratory

```{r compute cor matrix, echo=FALSE}
build_cor_mat <- function(df){
  cor_mat<-df %>% 
  select_if(is.numeric) %>% 
  drop_na() %>% 
  cor()

  n<-nrow(cor_mat)
  names<-row.names(cor_mat)
  correlation_tidy<-data.frame(row_name=character(),col_name=character(),corr=character())
  for(i in 1:n){
    
    for (l in i:n){
      corr<-cor_mat[i,l]
      row_name<-names[i]
      col_name<-names[l]
      
      df=data.frame(row_name,col_name,corr)
      correlation_tidy=rbind(correlation_tidy,df)
    }
  }
  return(list(cor_mat, correlation_tidy))
}

return_list <- build_cor_mat(train_df)
cor_mat <- return_list[[1]]
correlation_tidy <- return_list[[2]]
```

First we can analyze the correlation values between numerical variables. We can see the first few variable pairs with high correlation in the following table, which shows that many predictor variables have a strong linear relation with our response Sale Price. This informs our decision to fit a linear regression model, although we don't anticipate the linear regression model to perform best since the correlation values aren't extremely strong. This also suggests that a more complex model like trees or random forests may perform better at capturing this complex relationship.

```{r echo=FALSE}
# Check what has highest (absolute) correlation with sale price
correlation_tidy %>% 
  filter(col_name=="SalePrice") %>% 
  mutate(abs_corr=abs(corr)) %>%   
  arrange(desc(abs_corr))%>% 
  select(-abs_corr) %>% 
  filter(row_name != col_name) %>% 
  head(6)
```

If we look at all of the predictor variables' correlation with Sale Price, we can observe some counter-intuitive trends. For example, the fact that Overall Condition is negatively correlated with Sale Price may seem strange since one would anticipate the price to increase as the overall condition of the house improves. However, if we look closely at the plot between Sale Price vs Overall Condition, we can see that this is a consequence of trying to fit a linear model on a discrete numeric variable. This is another indication that linear regression may not be the best choice, and we need a more complex model to capture these type of complex trends.

```{r fig.height = 3, fig.width=5}
# Check which has negative correlation
# What Overall Cond has negative correlation?
correlation_tidy %>% 
  filter(col_name=="SalePrice",
         corr<=0) %>% 
  mutate(abs_corr=abs(corr)) %>% 
  arrange(desc(abs_corr))%>% 
  select(-abs_corr) %>% 
  head()

# SalePrice vs OverallCond
ggplot(train, aes(y = SalePrice, x = OverallCond)) + 
  geom_point() +
  labs(title = "Sale Price vs Overall Condition",
       y = "Overall Condition",
       x = "Sale Price")
```

Next, if we look at the correlation between numeric predictor variables, we will notice that there are many pairs with high correlation. Serious collinearity concerns make fitting a full linear model inadvisable, so we use subset selection methods and LASSO regression methods to address this issue. Tree-based methods, particularly random forests, are also used since they perform better on datasets with correlated predictors. We also try manually removing variables that have over 0.5 correlation and see if that improves the performance of our models.

```{r echo=FALSE}
# Correlation between predictors
correlation_tidy %>% 
  filter(col_name!="SalePrice",
         col_name!=row_name) %>% 
  mutate(abs_corr=abs(corr)) %>% 
  arrange(desc(abs_corr))%>% 
  select(-abs_corr) %>% 
  head()
```

The categorical variables are also problematic for linear regression, especially since some variables like Neighborhood have 24 different levels. Since linear regression creates dummy variables for each level of a categorical variable, linear regression on this data set will soon create numerous amounts of dummy variables. This will make the linear regression models more expensive to fit and harder to interpret, which further certifies the need of model selection methods.

```{r echo=FALSE}
sapply(lapply(train_df %>% select_if(negate(is.numeric)), unique), length)
```

Finally, we can investigate the distribution of the response variable Sale Price. The distribution is clearly right-skewed with outliers that have extremely high sale price values, so linear regression methods may suffer. Hence we consider a log transformation on the response variable to de-emphasize outliers and obtain a more normally-distributed response variable. Moreover, the residual plots confirm that doing a log transformation produces a residual plot with more constant variance and less skewness. This suggests that taking the log might improve linear model performance. However, we should also beware that the residuals still don't follow a normal distribution even after log transformation, as shown by the Normal QQ plot.

```{r fig.height = 3, fig.width=5, message=FALSE}
# SalePrice distribution is right skewed
ggplot(train, aes(x = SalePrice)) + 
  geom_histogram(color = "white", bins=30)+
  labs(title = "Distribution of Sale Price",
       y = "Sale Price")
```

```{r fig.height = 3, fig.width=5}
# SalePrice distribution is right skewed
ggplot(train, aes(x = log10(SalePrice))) + 
  geom_histogram(color = "white", bins=30) + 
  labs(title = "Distribution of Log transformed Sale Price",
       y = "Log(Sale Price)")
```

```{r echo=FALSE, warning=FALSE, fig.height = 3, fig.width=5}
# Taking log solves residual pattern
full_mod <- glm(SalePrice ~ ., data = train)
par(mfrow = c(1,2))
plot(full_mod, which = c(1,2))
mtext("Diagnostic plots for Sale Price", 
      side = 3,
      line = -2.5,
      outer = TRUE)
```

```{r echo=FALSE, warning=FALSE, fig.height = 3, fig.width=5}
par(mfrow = c(1,2))
log_mod <- glm(log10(SalePrice) ~ ., data = train)
plot(log_mod, which = c(1,2))
mtext("Diagnostic plots for Log-transformed Sale Price", 
      side = 3,
      line = -2.5,
      outer = TRUE)
```

# LR with Subset selection

### Reduce data

We compute the correlation matrix and use the findCorrelation function to drop variables that have over 0.5 pair-wise correlation. The way this function chooses which of the pair to drop is by computing its mean correlation value with all other variables, and dropping the one with a higher mean correlation value. We drop these variables in to create reduced train and test sets. The variables dropped are shown below.

```{r reduce data}

# correlation_tidy %>% 
#   filter(row_name != col_name) %>% 
#   filter(col_name != "SalePrice") %>% 
#   filter(abs(corr) > 0.5) %>% 
#   arrange(desc(abs(corr)))

# remove correlations with response variable
new_cor_mat <-cor_mat[rownames(cor_mat) != "SalePrice", colnames(cor_mat) != "SalePrice"]

# Finds pair-wise correlation over 0.5
# removes the one with higher mean correlation with other variables
rm_names <- findCorrelation(new_cor_mat, 0.5, names=T)
rm_names
rm_idx <- which(names(train) %in% rm_names)

# reduce train and test data
red_train = train[, -rm_idx]
red_test = test[, -rm_idx]

# # Check that reduced train_df has no variables with corr > 0.5
# red_return_list <- build_cor_mat(red_train_df)
# red_cor_mat <- red_return_list[[1]]
# red_correlation_tidy <- red_return_list[[2]]
# 
# red_correlation_tidy %>% 
#   filter(row_name != col_name) %>% 
#   filter(col_name != "SalePrice") %>% 
#   arrange(desc(abs(corr)))

# # Check that using train will result in removing the same variables
# tmp <- train %>% select_if(is.numeric) %>% cor()
# tmp_cor_mat <-cor_mat[rownames(tmp) != "SalePrice", colnames(tmp) != "SalePrice"]
# tmp_names <- findCorrelation(tmp_cor_mat, 0.5, names=T, verbose=T)
# sort(tmp_names) == sort(rm_names)
```

### Forward Selection

We use forward selection on usual data, reduced data, log-transformed data, and reduced log-transformed data.

```{r warning=FALSE, include=FALSE}
## R code: Model testing
n = 219
# n = dim(train)[2]
forward_mod <- regsubsets(SalePrice ~ ., data=train, nvmax=n, method="forward")
length(summary(forward_mod)$bic)
forward_mod_red <- regsubsets(SalePrice ~ ., data=red_train, nvmax=n, method="forward")
forward_mod_log <- regsubsets(log10(SalePrice) ~ ., data=train, nvmax=n, method="forward")
forward_mod_log_red <- regsubsets(log10(SalePrice) ~ ., data=red_train, nvmax=n, method="forward")
```

We choose the model with minimum BIC metric value, since that metric gives the simplest model. For example, the forward selection on the usual data gives the following models as the best according to different metrics.

```{r }
# Absolute metrics give full models as best, except bic.min which gives 64-model as best
adjr2.max <- which.max(summary(forward_mod)$adjr2)
rss.min <- which.min(summary(forward_mod)$rss)
cp.min <- which.min(summary(forward_mod)$cp)
bic.min <- which.min(summary(forward_mod)$bic)
data.frame(adjr2.max, rss.min, cp.min, bic.min)
```

Here is a plot of the BIC metric as model complexity increases. We can see that the minimum occurs at 40 variables.

```{r fig.height = 3, fig.width=5}
n = length(summary(forward_mod)$bic)

d <- data.frame(model = 1:(n),
adjr2 = summary(forward_mod)$adjr2,
rss = summary(forward_mod)$rss,
cp = summary(forward_mod)$cp,
bic = summary(forward_mod)$bic)

ggplot(d, aes(x = model, y = bic)) + 
  geom_line() +
  labs(title = "BIC metric for Forward Selection",
       x = "Number of variables in model",
       y = "BIC") + 
  geom_vline(aes(xintercept = bic.min),color = "red")
```

### Backward Selection

Similarly, we use backward selection on four types of data: whether or not it is reduced, and whether or not it is log-transformed.

```{r backward, warning=FALSE, include=FALSE}
backward_mod <- regsubsets(SalePrice ~ ., data=train, nvmax=n, method="backward")
backward_mod_red <- regsubsets(SalePrice ~ ., data=red_train, nvmax=n, method="backward")
backward_mod_log <- regsubsets(log10(SalePrice) ~ ., data=train, nvmax=n, method="backward")
backward_mod_log_red <- regsubsets(log10(SalePrice) ~ ., data=red_train, nvmax=n, method="backward")
```

We continue to use bic.min as the metric for model selection, since that gives the simplest model with 37.

```{r}
b_adjr2.max <- which.max(summary(backward_mod)$adjr2)
b_rss.min <- which.min(summary(backward_mod)$rss)
b_cp.min <- which.min(summary(backward_mod)$cp)
b_bic.min <- which.min(summary(backward_mod)$bic)
data.frame(b_adjr2.max, b_rss.min, b_cp.min, b_bic.min)
```

```{r fig.height = 3, fig.width=5}
b_d <- data.frame(model = 1:n,
adjr2 = summary(backward_mod)$adjr2,
rss = summary(backward_mod)$rss,
cp = summary(backward_mod)$cp,
bic = summary(backward_mod)$bic)

ggplot(b_d, aes(x = model, y = bic)) + 
  geom_line() + 
  labs(title = "BIC metric for Backward Selection",
       x = "Number of variables in model",
       y = "BIC") + 
  geom_vline(aes(xintercept = b_bic.min),color = "red")
```

### Predict

Next we make predictions and compute the test MSE values for each model.

```{r predict}
predict.regsubsets = function(object, newdata, id, ...) {
    form = as.formula(object$call[[2]])
    mat = model.matrix(form, newdata)
    coefi = coef(object, id = id)
    mat[, names(coefi)] %*% coefi
}

lr_results <- function(model, reduce=F, log=F){
  id = which.min(summary(model)$bic)
  preds <- predict.regsubsets(model, test, id= id)
  if(log){
    preds <- 10^preds
  }
  if(reduce){
    mse <- mean((red_test$SalePrice - preds)^2)
  } else{
    mse <- mean((test$SalePrice - preds)^2)
  }
  bic <- summary(model)$bic[id]
  r2 <- summary(model)$rsq[id]
  adjr2 <- summary(model)$adjr2[id]
  df <- data.frame(mse, bic, r2, adjr2)
  return(df)
}

lr_out <- rbind(
  lr_results(forward_mod),
  lr_results(forward_mod_red, reduce = T),
  lr_results(forward_mod_log, log=T),
  lr_results(forward_mod_log_red, reduce = T, log=T),
  lr_results(backward_mod),
  lr_results(backward_mod_red, reduce = T),
  lr_results(backward_mod_log, log=T),
  lr_results(backward_mod_log_red, reduce = T, log=T)
) %>% mutate(
  Algorithm = c("Forward", "Forward Reduced", "Forward Log", "Forward Log Reduced",
                "Backward", "Backward Reduced", "Backward Log", "Backward Log Reduced")
) %>% 
   rename(
     Test_MSE = mse,
     BIC = bic,
     R_squared = r2,
    Adjusted_R_squared = adjr2) %>% 
   mutate(Test_RMSE = sqrt(Test_MSE)) %>% 
   select(Algorithm, Test_MSE, Test_RMSE, BIC, R_squared, Adjusted_R_squared)
```

We can see that the log transformation reduces test MSE value but reducing the data set doesn't and will instead increase test MSE. This confirms our conjecture in the exploratory state that log transformation will improve model performance. It also reveals that manually removing highly correlated variables don't actually improve model performance, and it is better to let the subset selection methods to their job.

Overall, backward selection seems to outperform forward selection. Hence according to test MSE, $R^2$, and adjusted $R^2$, backward selection on log-transformed data performs best. However, the BIC metric is minimized for backward selection on reduced data. Nevertheless, we decide to use backward selection on log-transformed data since the majority of the metrics agree on this algorithm.

```{r}
names(lr_out)[5] = "$R^2$"
names(lr_out)[6] = "Adjusted $R^2$"
lr_out %>%
  pander::pander()
```

Here are the 37 variables included in the best model under backward selection on log-transformed model. We can see that many variables are related to measurements and qualities of various parts of the house, such as the bathroom, kitchen, and basement; as well as quality of amenities such as the garage and the fireplace. House price is also impacted if the house is located in nine specific neighborhoods.

```{r warning = FALSE, include=FALSE}
coef_back <- coef(backward_mod_log, id = which.min(summary(backward_mod_log)$bic))
```

```{r echo=F}
coef_back
```

# LASSO regression

We apply LASSO regression on both the usual and logged-transformed data. We perform cross validation to compare the MSE across all values of $\lambda$. Below are the plots for MSE vs $\lambda$ values for both models:

```{r fig.height = 3, fig.width=5}
set.seed(1)

x<-model.matrix(SalePrice ~., data = train)[,-1]
y<-train$SalePrice

logx<-model.matrix(log10(SalePrice) ~., data = train)[,-1]
logy<-log10(train$SalePrice)

grid = 10^(seq( -3, 5, length = 100))
log_grid = seq( -3, 5, length = 100)
lasso_cv<-cv.glmnet(x,y,alpha=1,lambda=grid,nfolds=10)
log_lasso_cv<-cv.glmnet(logx,logy,alpha=1,grid = log_grid, nfolds=10)
plot(lasso_cv)
mtext("CV error vs lambda value for Sale Price", 
      side = 3,
      line = -1,
      outer = TRUE)
```

```{r fig.height = 3, fig.width=5}
plot(log_lasso_cv)
mtext("CV error vs lambda value for Log-transformed Sale Price", 
      side = 3,
      line = -1,
      outer = TRUE)
```

For our optimal $\lambda$ values, we set the $\lambda$ to values that minimize CV error for each model:

```{r e}
min_L= lasso_cv$lambda.min
log_min_L = log_lasso_cv$lambda.min
data.frame(min_L, log_min_L)
```

We make predictions and compute the test MSE values for the LASSO and LASSO Log models.

```{r}
lasso_mod<-glmnet(x,y,alpha=1,lambda=grid,nfolds=10)
test_mat<-model.matrix(SalePrice ~., data = test)[,-1]
lasso_pred<-predict(lasso_mod,s=min_L, newx=test_mat)

log_lasso_mod<-glmnet(logx,logy,alpha=1,lambda=grid,nfolds=10)
log_test_mat<-model.matrix(log10(SalePrice) ~., data = test)[,-1]
log_lasso_pred<-predict(log_lasso_mod,s=log_min_L, newx=log_test_mat)
```

We see that for LASSO regression, the log transformation also improves test RMSE by 7.1%, as expected in the explanatory section. We thus choose the logged model for LASSO.

```{r}
test_TV<-test$SalePrice
lasso_MSE<-mean((test_TV-lasso_pred)^2)
log_lasso_MSE<-mean((test_TV-10^(log_lasso_pred))^2)
 
lasso_out <- data.frame(Algorithm=c("LASSO", "LASSO Log"),Test_MSE=c(lasso_MSE, log_lasso_MSE)) %>%
  mutate(Test_RMSE=sqrt(Test_MSE))%>% 
  arrange(desc(Test_MSE))

lasso_out %>% pander::pander()
```

The Log LASSO model selects 220 out of 261 levels. Below are the 15 selected levels with highest coefficients:

```{r}
level <- data.frame(matrix(ncol = 75, nrow = 1))
for (i in 1:75){
  if(typeof(train[[i]]) == "character"){
    level[[i]] = as.factor(train[[i]]) %>%
    nlevels()
  } else {
    level[[i]] = 1
  }
}

total_level <- rowSums(level)

s <- which(lasso_mod$lambda==min_L)
coef <- data.frame(coef = coef(lasso_mod)[,s]) %>%
  arrange(desc(coef))
coef_print <- coef %>%
  head(15)
coef_print
```

Since the coefficients of each level do not indicate its significance, given the large number of levels, we won't do feature analysis here.

# Tree-based methods

### Training

We start with a single pruned decision tree. First we train a full tree. Next, we pick the maximum value of CP that has $X$-relative error within one standard deviation of the mean. This value balances performance and complexity. Then we prune our tree with the optimal CP value to make our final tree. With the pruned regression tree has been fit, we then fit bagged and random forests. First we fit the bagged tree by letting `mtry` be equal to the number of predictors. Then we fit the random forest. Here we set `mtry` equal to the default number for a random forest for regression ($p/3$). Then we extract predictions on test data and create a table of the test MSEs for each model to get a preliminary idea of the relative strengths of each model.

```{r}
# Code has been manually cached below for parity between machines

pruned<-readRDS("pruned.rds")

# # First we train a full tree.
# set.seed(1)
# T0<-rpart(SalePrice~., data=train, 
#              control = rpart.control(cp=0))
# 
# # Next, we pick the maximum value of CP that has 
# # $X$-relative error within one standard deviation of the mean. 
# # This value balances performance and complexity
# cptable<-T0$cptable %>% as.data.frame()
# 
# 
# p.rpart <- T0$cptable
# xstd <- p.rpart[, 5L]
#     xerror <- p.rpart[, 4L]
# minpos <- min(seq_along(xerror)[xerror == min(xerror)])
# thresh<-(xerror + xstd)[minpos]
# 
# 
# best_cp<-cptable %>% 
#   filter(xerror<=thresh) %>% 
#   filter(CP==max(CP)) %>% 
#   select(CP) %>% 
#   pull()
#   
# 
# pruned<-prune(T0,best_cp)
# saveRDS(pruned,file="pruned.rds")
```

```{r}
#First we fit the bagged tree, letting `mtry` be equal to the number of predictors

bag<-readRDS("bag.rds")
# set.seed(1)
# bag<-randomForest(SalePrice~.,
#                        mtry=ncol(train)-1,
#                        data=train,
#                        importance=T)
# saveRDS(bag,file="bag.rds")


log_bag<-readRDS("log_bag.rds")
# set.seed(1)
# log_bag<-randomForest(log10(SalePrice)~.,
#                        mtry=ncol(train)-1,
#                        data=train,
#                        importance=T)
# saveRDS(log_bag,file="log_bag.rds")
```

```{r}
#Now we fit the random forest. Here we set `mtry` equal to the 
# default number for a random forest for regression ($p/3$).

rf<-readRDS("rf.rds")
# set.seed(1)
# rf<-randomForest(SalePrice~.,
#                        mtry=(ncol(train)-1)/3,
#                        data=train,
#                        importance=T)
# saveRDS(rf,file="rf.rds")

log_rf<-readRDS("log_rf.rds")
# set.seed(1)
# log_rf<-randomForest(log10(SalePrice)~.,
#                        mtry=(ncol(train)-1)/3,
#                        data=train,
#                        importance=T)
# saveRDS(log_rf,file="log_rf.rds")
```

```{r}
## Now we extract predictions on test data and create a table of MSEs for each model.

#Tree preds
test_Tree_pred<-predict(pruned,newdata = test
                          )

#RF
test_rf_preds<-predict(rf,test)

#Bagged
test_bag_preds<-predict(bag,test)

#RF
log_rf_preds<-10^predict(log_rf,test)

#Bagged
log_bag_preds<-10^predict(log_bag,test)

#True values
test_TV<-test$SalePrice
```

```{r}
# Get MSEs for Trees
tree_MSE<-mean((test_TV-test_Tree_pred)^2)
rf_MSE<-mean((test_TV-test_rf_preds)^2)
bag_MSE<-mean((test_TV-test_bag_preds)^2)
log_rf_MSE<-mean((test_TV-log_rf_preds)^2)
log_bag_MSE<-mean((test_TV-log_bag_preds)^2)

data.frame(Algorithm=c("Pruned Tree","Bagged Forest","Bagged Forest Log", 
                       "Random Forest","Random Forest Log"),
           Test_MSE=c(tree_MSE,bag_MSE,log_bag_MSE,rf_MSE,log_rf_MSE)) %>% 

mutate(Test_RMSE=sqrt(Test_MSE))%>% 
  pander::pander()
```

Among trees, random forest performs the best and the pruned tree performs the worst. Models with log transforms also tend to perform worse on average than those without. This is unsurprising, as we would not expect a log transformation to increase tree performance. The fact that the pruned tree performs worse than the other trees makes sense since pruned trees are notorious for overfitting. The random forest also performs about $3\%$ better than the bagged tree. Whether this increase in performance is significant is subject to debate, but we would expect a random forest to outperform a bagged tree on these data. This is because, returning to our EDA, many of the predictors in this dataset are highly correlated, which means that a bagged tree, which has access to all of the predictors at every step, might systematically choose non-optimal trees due to its greedy selection algorithm. In addition to predictions, the random forest also gives us access to variable importance data which we investigate now.

### Variable Importance

```{r fig.height = 3, fig.width=5}
library(RColorBrewer)
imp<-importance(rf) %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  rename("Predictor"=rowname) %>% 
  arrange(desc(`%IncMSE`)) %>% 
  mutate(Feature_Type=case_when(Predictor%in%c("GrLivArea", "TotalBsmtSF", "GarageArea", "X2ndFlrSF","X1stFlrSF","LotArea","BsmtFinSF1")~"Square Footage",
                        Predictor%in%c("OverallQual", "ExterQual", "BsmtFinSF1", "GarageFinish","BsmtFinType1")~"Quality",
                        Predictor=="YearBuilt"~"Age",
                        Predictor%in%c('GarageType',"CentralAir")~"Amenities",
                        T~"Other")) %>% 
  mutate(Feature_Type=fct_relevel(Feature_Type, levels=c("Square Footage","Quality","Age","Amenities", "Other"))) %>% 
  mutate(Predictor=case_when(Predictor=="X1stFlrSF"~"1stFlrSF",
                             Predictor=="X2ndFlrSF"~"2ndFlrSF",
                             T~Predictor))


imp[1:15,] %>% 
  mutate(Predictor = fct_reorder(Predictor, `%IncMSE`)) %>% 
  ggplot( mapping=aes(x=Predictor,y=`%IncMSE`,fill=Feature_Type))+
  geom_col(color="white")+
  coord_flip()+
  scale_fill_brewer(palette="Pastel1")+
  labs(title="Change in error when predictors are removed",y="Percent Increase in MSE",fill="Feature Type")

```

Here the metric printed in the table is the percent increase in out of bag MSE when the values of a predictor are permuted. This metric provides a fairly robust estimate of the relative importance of predictors in our dataframe. The most important predictor is General Living Area. After that the next most important predictor is Overall Quality, a rating of the overall material and finish of the house on a scale one to ten. Year Built comes in 4th place, and many of the remaining important predictors have to do with the floor area and quality of various parts of the house. The important predictors are grouped by their general category. The major categories are "Square Footage," "Quality," "Age," and "Amenities." The "Square Footage" category of variables deals with the footprint of various features of the house from the floors and basement to the garage to the lot size. The "Quality" predictors are metrics with deal with the material quality and state of repair of features of the home. "Age" is self explanatory. "Amenities" concerns additional features of the home such as a garage and central air conditioning. All but one of the 15 most important features fall into these four categories. The one other important feature of note is `MSZoning`.According to the codebook,`MSZoning` pertains to the general zoning classification of the sale (ie Commercial, Agricultural, Residential High Density, and so on). The importance of this variable seems to indicate that differently zoned transactions have different Sale Prices.

### Further tuning

Now that we have a good image of the output of one random forest, we continue tuning to see if we can eek out more performance by tuning some of the hyperparameters that are used when fitting random forests. We tune the number of predictors used when splitting (`mtry`), the minimum node size for terminal nodes (`nodesize`), and the number of trees in the forest (`ntree`). In order to minimize computation time, we perform a guided and greedy search, first optimizing `mtry`, then optimizing `nodesize` for the optimal value of `mtry`, then optimizing `ntree` for using the optimal values of `mtry` and `nodesize`. Other less greedy approaches were attempted, but they took several hours to converge and ultimately led to very similar parameter values. Thus the greedy approach is presented in this report.

We first estimate test error and OOB error as a function the `mtry` parameter (the number of predictors used in splitting). The test MSE and OOB MSE minimizing values for `mtry` are printed below along with the minimal value of `mtry` one standard error above the minimum:

```{r}

 m_try_tune<-readRDS("m_try_tune.rds")

#This Allows us to compute the tuning grid directly (Takes about 10 minutes) 


# library(tictoc)
# tic()
# m_try_tune<-list()
# 
# for (i in 1:(ncol(train)-1)){
#   set.seed(1)
#   m_try_tune[[i]]<-randomForest(SalePrice~.,
#                        mtry=i,
#                        data=train,
#                        importance=T)
# }
# saveRDS(m_try_tune,file="m_try_tune.rds")
# toc()
```

```{r}
mtry_oob<-double(length=ncol(train)-1)
mtry_tMSE<-double(length=ncol(train)-1)
mtry<-1:(ncol(train)-1)
                 
for (i in 1:(ncol(train)-1)){
  mtry_oob[i]=mean(m_try_tune[[i]]$mse)
  
  mtry_tMSE[i]<-mean((test_TV-predict(m_try_tune[[i]],test))^2)
}
mtry_df<-data.frame(mtry,mtry_tMSE,mtry_oob)
```

```{r}
se_mtry_oob=sd(mtry_df$mtry_oob)
se_mtry_tMSE=sd(mtry_df$mtry_tMSE)


data.frame(`Error_Type`=c("Test MSE", "OOB Error"),
           `Minimal_mtry_value`=c(which.min(mtry_df$mtry_tMSE),
                                  which.min(mtry_df$mtry_oob)),
           `Min_mtry_1se`=c(mtry_df %>%
                              filter(mtry_tMSE<=min(mtry_tMSE)+se_mtry_tMSE) %>% 
                              filter(mtry==min(mtry)) %>% 
                              select(mtry) %>% 
                              pull(),
                            mtry_df %>%
                              filter(mtry_oob<=min(mtry_oob)+se_mtry_oob) %>% 
                              filter(mtry==min(mtry)) %>% 
                              select(mtry) %>% 
                              pull()
                            )) %>% 
  pander::pander()
```

Initially, in keeping with the principle of parsimony, we selected the minimal value of `mtry` one standard error above the minimum as our optimal value. However, this value of `mtry` was small to the point that it created models so biased that test MSE increased by $10\%$. This is clearly non-optimal, so we turn to analysis of the RMSE-`mtry` plot to find the optimal `mtry` value. This error plot is shown below:

```{r  fig.height=3.5,fig.width=8}
#LEAVE THE FIGURE DIMENSIONS AS IS!!!!
ggplot(mtry_df %>% 
  pivot_longer(2:3,names_to = "Metric",values_to = "Error") %>% 
    mutate(Metric=case_when(Metric=="mtry_oob"~"OOB RMSE",
                            Metric=="mtry_tMSE"~"Test RMSE")),
  aes(x=mtry,y=sqrt(Error),color=Metric))+
  geom_line()+
  
  labs(y="RMSE",
       x="Number of Predictors Used In Each Split",
       title="Random Forest Prediction Error vs. Number of Predictors Used in Each Split")
```

This plot is rather noisy, so we fit a `geom_smooth` to the data in order with `method = loess` to get a better idea of the general trends in the data.

```{r   fig.height=3.5,fig.width=8, message=FALSE}
#LEAVE THE FIGURE DIMENSIONS AS IS!!!!
ggplot(mtry_df %>% 
  pivot_longer(2:3,names_to = "Metric",values_to = "Error") %>% 
    mutate(Metric=case_when(Metric=="mtry_oob"~"OOB RMSE",
                            Metric=="mtry_tMSE"~"Test RMSE")),
  aes(x=mtry,y=sqrt(Error),color=Metric))+
  geom_line()+
  geom_smooth(se=F)+
  geom_vline(xintercept = 30)+
  labs(y="RMSE",
       x="Number of Predictors Used In Each Split",
       title="Random Forest Prediction Error vs. Number of Predictors Used in Each Split")
```

With the `geom_smooth` fit, the general trends in the error are more obvious. Both curves have elbows around `mtry = 20`. After these elbows there is some variance, but both curves seem to level out by `mtry = 30`. After `mtry = 40` for test RMSE and `mtry = 50` for OOB RMSE, the curves begin to increase. This reproduces the observation above that bagged forests seem to perform worse than random forests on these data. With this curvature in mind, we take `mtry = 30` as our optimal parameter value.

Next, we use our optimal number for `mtry` and tune the minimum terminal node size for our trees. The test MSE and OOB MSE minimizing values for `nodesize` are printed below:

```{r}

 node_size_tune<-readRDS("node_size_tune.rds")

#This Allows us to compute the tuning grid directly (Takes about 10 minutes) 

 node_size<-1:100

 # tic()
 # node_size_tune<-list()
 # 
 # for (i in 1:length(node_size)){
 #   set.seed(1)
 #   node_size_tune[[i]]<-randomForest(SalePrice~.,
 #                        mtry=30,
 #                        data=train,
 #                        importance=F,
 #                        nodesize=node_size[i])
 # }
 # saveRDS(node_size_tune,file="node_size_tune.rds")
 # toc()
```

```{r}
node_size_oob<-double(length=length(node_size))
node_size_tMSE<-double(length=length(node_size))

                 
for (i in 1:length(node_size)){
  node_size_oob[i]=mean(node_size_tune[[i]]$mse)
  
  node_size_tMSE[i]<-mean((test_TV-predict(node_size_tune[[i]],test))^2)
}

node_size_df<-data.frame(node_size,node_size_tMSE,node_size_oob)
```

```{r}
data.frame(`Error_Type`=c("Test MSE", "OOB Error"), 
           `Minimal_nodesize_value`=c(which.min(node_size_df$node_size_tMSE),
                                      which.min(node_size_df$node_size_oob))) %>% 
  pander::pander()
```

To get a better picture of the relationship between the error and `nodesize`, we plot the errors below:

```{r   fig.height=3.5,fig.width=8, message=FALSE}
ggplot(node_size_df %>% 
  pivot_longer(2:3,names_to = "Metric",values_to = "Error")%>% 
    mutate(Metric=case_when(Metric=="node_size_oob"~"OOB RMSE",
                            Metric=="node_size_tMSE"~"Test RMSE")),
  aes(x=node_size,y=sqrt(Error),color=Metric))+
  geom_line()+
  geom_smooth(se=F)+
  labs(y="RMSE",x='Node Size',
       title="Random Forest Prediction Error vs. Terminal Node Size")
```

The OOB error minimizing and test MSE minimizing values agree on a minimal node size of one. In order to make sure we are not just fitting to noise, we fit a `geom_smooth` to the data in order with `method = loess` to get a better idea of the general trends in the data. The curve made it easier to see that both errors were varying fairly evenly around a strictly increasing trend. Hence we take the minimal value of one for our optimal parameter value. Lending credence to this choice is the fact that a value of 1 is the default value that `randomForest` uses for regression.

Finally we use all of our optimized parameters while fitting a forest and tune the number of trees. Really we are checking whether the `randomForest` default of 500 trees is enough for the test and OOB MSE to converge for this dataset. Again we print the test MSE and OOB MSE minimizing values for `nodesize` are in a table below:

```{r}

 n_tree_tune<-readRDS("n_tree_tune.rds")

#This Allows us to compute the tuning grid directly (Takes about an hour) 


 tree_num<-seq(20,1000,by=20)

# tic()
# n_tree_tune<-list()
# 
# for (i in 1:length(tree_num)){
#   set.seed(1)
#   n_tree_tune[[i]]<-randomForest(SalePrice~.,
#                        mtry=30,
#                        data=train,
#                        nodesize=1,
#                        ntree=tree_num[i],
#                        importance=F)
# }
#  saveRDS(n_tree_tune,file="n_tree_tune.rds")
# toc()
```

```{r}
ntree_oob<-double(length=length(tree_num))
ntree_tMSE<-double(length=length(tree_num))

                 
for (i in 1:length(tree_num)){
  ntree_oob[i]=mean(n_tree_tune[[i]]$mse)
  
  ntree_tMSE[i]<-mean((test_TV-predict(n_tree_tune[[i]],test))^2)
}
ntree_df<-data.frame(tree_num,ntree_tMSE,ntree_oob)
```

```{r}

se_ntree_oob=sd(ntree_df$ntree_oob)
se_ntree_tMSE=sd(ntree_df$ntree_tMSE)
data.frame(`Error_Type`=c("Test MSE", "OOB Error"), 
           `Minimal_ntree_value`=c(which.min(ntree_df$ntree_tMSE)*20,
                                   which.min(ntree_df$ntree_oob)*20),
            `Min_ntree_1se`=c(ntree_df %>%
                              filter(ntree_tMSE<=min(ntree_tMSE)+se_ntree_tMSE) %>% 
                              filter(tree_num==min(tree_num)) %>% 
                              select(tree_num) %>% 
                              pull(),
                            ntree_df %>%
                              filter(ntree_oob<=min(ntree_oob)+se_ntree_oob) %>% 
                              filter(tree_num==min(tree_num)) %>% 
                              select(tree_num) %>% 
                              pull()
                            )) %>% 
pander::pander()
```

Here we see that the Test MSE is actually minimized for an `ntree` value of 40. Yet this likely represents noise as opposed to signal. Following the conventions of ISLR, with `ntree`, we are less interested in the precise number of trees which minimizes error, and more interested in the number of trees beyond which the values begin to level off. In order to find this value, we must look to the plot.

```{r  fig.height=3.5,fig.width=8, message=FALSE}
ggplot(ntree_df %>% 
  pivot_longer(2:3,names_to = "Metric",values_to = "Error")%>% 
    mutate(Metric=case_when(Metric=="ntree_oob"~"OOB RMSE",
                            Metric=="ntree_tMSE"~"Test RMSE")),
  aes(x=tree_num,y=sqrt(Error),color=Metric))+
  geom_line()+
  labs(y="RMSE",x='Number of Trees',
       title="Random Forest Prediction Error vs. Number of Trees")
```

The OOB-Error seems to asymptotically decrease with the number of trees. The Test MSE is more variable at the beginning but then begins to level out around 750 trees. Following the conventions discussed ISLR, we want to pick a number of trees threshold past which the plot MSE seems to converge. There is a hard elbow in the OOB MSE plot around `ntree = 200`, after which the decrease in MSE seems to level off. In earlier tuning, we tested ntree values up to 2000 and found that doubling the number of trees from 1000 to 2000 decreased the OOB RMSE by about 0.59%. This showed that there are serious diminishing returns at play for more than 1000 trees. The fact that the minimal number of trees with an OOB MSE within one standard deviation of the minimum is 160 again shows that the MSE curves really start to level out past `ntree = 200`. Since ISLR says that overfitting is not a concern with the number of trees and the RMSE curves level off past 200, we take the standard `ntree = 500` value for random forest.

Now we can fit the final random forest, with `mtry = 30`, `nodesize = 1`, and `ntree = 500`. We can compare the performance this final random forest to the untuned forest using the table below:

```{r}
 final_rf<-readRDS("final_rf.rds")
# set.seed(1)
# final_rf<-randomForest(SalePrice~.,
#                        mtry=30,
#                        data=train,
#                        nodesize=1,
#                        ntree=500,
#                        importance=F)
# saveRDS(final_rf,file="final_rf.rds")

```

```{r}
rf_oob=mean(rf$mse)
rf_MSE<-mean((test_TV-test_rf_preds)^2)
final_rf_oob= mean(final_rf$mse) 
final_rf_tMSE<-mean((test_TV-predict(final_rf,test))^2)

final_rf_sum<-data.frame(final_rf_oob,final_rf_tMSE) %>% 
  pivot_longer(1:2,names_to = "Metric",values_to = "MSE") %>% 
  mutate(Random_Forest="Tuned")

table<-data.frame(rf_oob,rf_MSE) %>% 
  pivot_longer(1:2,names_to = "Metric",values_to = "MSE") %>% 
  mutate(Random_Forest="Untuned")
rbind(final_rf_sum,table) %>% 
  mutate(RMSE=sqrt(MSE)) %>% 
  select(-MSE) %>% 
  mutate(Metric=case_when(Metric=="final_rf_oob"~"OOB RMSE",
                          Metric=="rf_oob"~"OOB RMSE",
                          Metric=="final_rf_tMSE"~"Test RMSE",
                          Metric=="rf_MSE"~"Test RMSE")) %>% 
  pivot_wider(names_from = "Random_Forest",values_from = "RMSE") %>% 
  select(Metric,Untuned,Tuned) %>% 
  mutate(Percent_Change=100*(Tuned-Untuned)/Untuned) %>% 
  pander::pander()
  
```

We see that all of that time tuning resulted in around a one point two percent decrease in OOB RMSE and a decrease in Test RMSE of 0.9%. Both decreases are so marginal that the statistical and practical significance of the decrease is questionable. We take the tuned forest as the best model among tree models, but it is clear that our guided search tuning process was not incredibly effective in producing a model with lower predicted error on unseen data.

```{r}
#Here are the final test MSE's if someone wants to use them in a table
#Original RF
rf_oob=mean(rf$mse)
rf_MSE<-mean((test_TV-test_rf_preds)^2)

#Tuned RF
final_rf_oob= mean(final_rf$mse) 
final_rf_MSE<-mean((test_TV-predict(final_rf,test))^2)

tree_out <- data.frame(Algorithm=c("Pruned Tree","Bagged Forest", 
                                   "Random Forest", "Tuned Random Forest"),
                       Test_MSE=c(tree_MSE,bag_MSE,rf_MSE, final_rf_MSE)) %>% 
mutate(Test_RMSE=sqrt(Test_MSE))
```

# Conclusion

```{r}
rbind(
lr_out %>% filter(Algorithm == "Backward Log") %>% select(Algorithm, Test_RMSE, Test_MSE),
lasso_out  %>% filter(Algorithm == "LASSO Log"),
tree_out  %>% filter(Algorithm == "Tuned Random Forest")) %>% 
  arrange(Test_RMSE) %>% pander::pander()
```

Now we compare all the models that perform the best in each model type. From the table we see that the Tuned Random Forest model outperforms all other models with RMSE 24759, we thus select this model to perform prediction on the test set. Noted that the LASSO Log model has only a 3.4% higher RMSE than the Random Forest. Given a large number of predictors, it's surprising how well the LASSO Log model performs against the random forest model. If our goal is more about inference instead of prediction, LASSO Log would be preferred since it is more interpretable and provides coefficients for each level.

To answer our research question on which features are the most relevant in predicting house sale price, we compare the levels selected by each model. The backward selection on log-transformed model selects 43 out of 219 levels (a categorical variable with n levels would be coded into n-1 dummy variables), the lasso regression on log-transformed model selects 220 out of 261 levels, and the tuned random forest model provides an estimate of the relative importance of predictors. We use the variable importance estimate given by the tree model as a reference and check whether the most important variables in the table are also selected by both the backward selection and the LASSO model. In general, the key features in determining house price are the size and quality of each area of the house, the type of amenities available, the type of neighborhood, and the age of the house. The specific variables that are the most relevant are :`GrLvArea`, `2ndFlrSF`, `TotalBsmtSF`, `OverallQual`, `ExterQual`, `GarageType`, `CentralAir`,`Neighborhood`,`YearBuilt`.

# References

Montoya, Anna. 2016. "House Prices - Advanced Regression Techniques." Kaggle. Retrieved December 5, 2022 (<https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/>). \newpage \# Code Appendix

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
